Для построения нода, нужно пройтись по столбцам данных и делить их каждый раз на две группы, выбирая каждый раз одно среднее между двумя уникальными отсортированными значениями. 
далее мы по этому разделению выбираем 2 группы таргетов и считаем:
$$
Information Gain = S_{0}- \sum\limits_{i=1}^{N}{\frac{N_i}{N}S_{i}} \rightarrow max
$$
где S нулевое - [[Entropy]] неразделенных таргетов выборки.
Выбрав критерий с наибольшим приростом информации, делим данные на 2 части, создавая, таким образом нод дерева. Повторять до тех пор, пока не достигнем останавливающих условий.

Это всё было для задачи классификации, в задаче регрессии вместо прироста информации используется среднее суммы квадратов остатков (как в [[R squared metric]])
и сплит делается по порогу с наименьшей суммой. Тоесть мы сплитим и считаем средние двух групп. 

Чтобы избежать оверфита, листья деревьям обычно обрубают. 
Для этого оверфитное дерево обрезается, пока не останется один корневой нод, и высчитывается 
$$
TreeScore = SSR + \alpha T
$$
где ssr - sum of squared residuals, a - параметр, который нужно подбирать, T - колличество листьев. В итоге из всех обрезаных деревьев остается одно с наименьшим скором.
Альфа подбирается кроссвалидацией. Т. е. сторим полноде дерево и потом подбираем кросвалидацией лучшее обрезанное дерево. Но до этого все значимые альфа значения находятся путем увеличения, пока скор нового обрезанного дерева не станет меньше скора предыдущего обрезанного или полного дерева. 
 Для классификации алгоритм будет почти такимже.