Gradient Boost Machine

отличается от adaboost тем, что строит деревья побольше, чем пеньки
Градиентным он назван потому, что в нем нужно вычислять разницу между значениями y и значениями fx деревьев.
### Регрессия
алгоритм работы:
1) находим среднее значение всех y
2) находим разности между y и средним
3) строим дерево, которое будет предсказывать разность из пункта 2
4) сложить среднее и результаты предсказаний дерева, умноженное на learning rate
5) повторить
### Классификация
алгоритм:
1) Найти вероятнось положительного класса при помощи формулы:
		*log(кол-во положительных / колво отр.)*, подставленное в сигмоиду: ![{\displaystyle \sigma (x)={\frac {1}{1+e^{-x}}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b2a78e39c42d7d51c4041d142740a7719e55b314)
2) получить разницу между *y* и вероятностью
3) построить дерево по разницам
4) при сложении, все листья нужно прогнать через формулу:
*сумма отстатков / сумму произведения предыдущих вероятностей и (1 - предыдущих вероятностей)*,
а затем умножить на лернинг рэйт и сложить с предыдущими результатами
4)  результаты прогнать через сигмоиду
5) повторить