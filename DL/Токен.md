Атомарный элемент последовательности. Может быть словом, символом предложением и тд. 
Большая часть алгоритмов токенизации разбиваются на 2 части:
1) [[Token learner]]
2) [[Token segmenter]]

Существуют два популярных алгоритма токенизации:
1) [[Byte-Pair Encoding]]
2) [[Unigram Language Modeling]]
Еще есть [[SentencePiece]] но он служит усреднением первого.