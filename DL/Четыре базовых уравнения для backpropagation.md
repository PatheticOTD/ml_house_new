$\delta_j^l$ - ошибка на j-том нейроне l-ого слоя. 
Метод обратного распространения ошибки поможет нам вычислить эту самую ошибку и затем соотнести ее с $\frac{dC}{dw_{jk}^l}$ и с такой же производной по bias - у

Чем больше эта ошибка, тем больше производная $\frac{dC}{dz_j^l}$ и наоборот. Соответственно ошибка эквивалентна производной C по z j-ого нейрона l-ого слоя.
$\delta^l$ -  вектор ошибок слоя l. бэкпропогэтион даст нам способ вычислить этот самый вектор для каждого слоя и затем соотнести эти ошибки с частными производными  $\frac{dC}{dw_{jk}^l}$ и такой же производной по bias - у.

#### 1) Уравнение ошибки
![[Screenshot from 2024-08-14 16-53-24.png]]
производная C по a показывает то, на сколько конкретный нейрон влияет на ответ сети. Чем это значение больше, тем влиятельней этот нейрончик.

Расскрывая эту формулу подробнее:
![[Screenshot from 2024-08-14 17-05-04.png]]
эта формула используется на начальном этапе, те на самом последнем слое.
#### 2) Уравнение ошибки относительно следующего слоя
![[Screenshot from 2024-08-14 17-05-25.png]]
Эта же для слещующих с конца слоев.

#### 3) Уравнение изменения функции потерь относительно любого баеса в сети
![[Screenshot from 2024-08-14 17-19-51.png]]

#### 4) Уравнение изменения функции потерь относительно любого веса в сети
![[Screenshot from 2024-08-14 17-24-59.png]]
