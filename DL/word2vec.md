 [[tf-idf]] и [[PPMI]] в результате дают очень длинные вектора с данными, разбросанными по всем измерениям, что плохо влияет на производительность и точность нейронных сетей. Идея, стоящая за алгоритмом заключается в тренировке классификатора, предсказывающего появления слова рядом с контекстным словом. Но вместо пресказывания появления, он будет предсказывать веса, как эмбединги для слова. За основу модели берется логистическая регрессия.
 
 Эмбединги для слов статичны, что означает то, что для каждого слова алгоритм будет выдавать один и тот же
 эмбединг.

word2vec использует skip-gram with negative sampling.
Идея скип-грамы состоит в следующем:
1) относиться к целевым словам и к соседним контекстным словам, как к позитивным сэмплам
2) Случайно выбрать слова из лексикона для получения негативных сэмплов
3) использовать логистическую регрессию для тренировки класификатора
4) использовать выученые веса, как эмбеденги.

#### Классификатор
для каждого целевого слова выбирается контекстное окно (несколько слов справа и слева от слова).
далее расчитывается вероятность появления этого слова рядом с контектстным при помощи матричного умножения и сигмоиды.
$$
P(+|w,c) =\sigma(-c*w)= \frac{1}{1+ exp(-c*w)}
$$
$$
P(-|w,c) =\sigma(-c*w)= \frac{1}{1+exp(c*w)}
$$
Скип-грамма делает упрощающее жизнь предположение о том, что все контекстные слова независимы, что позволяет нам их перемножить:
 $$
 P(+|w,c_{1:L}) = \prod\sigma(c*w)
$$
$$
 \log P(+|w,c_{1:L}) = \sum\limits_{i=1}^L\log\sigma(c*w)
$$
Чтобы все это посчитать, нам прото нужны эмбеденги.

#### Learning skip-gram embeddings
Алгоритму скармливается корпус слов и словарь размера N. Затем к каждому слову присваивается случайный эмбединг, который потом будет меняться итерационно.

Негативных сэмлов должно быть больше, чем позитивных. Негативные сэмплы выбираются по модифицированной вероятности, показанной в [[PPMI]], где альфа всё также будет равно 0.75, так как повышает вероятности у редких слов.

Далее алгоритм тренеруется максимизировать похожесть похожих слов и непохожесть непохожих слов.