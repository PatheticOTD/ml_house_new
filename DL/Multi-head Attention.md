Каждая атеншн бошка ([[Self-Attention]]) находится на одном слое с другими, а значит у каждой головы свои матрицы ключей, значений и квери. Все скоры голов конкатенируются и умножаются на матрицу весов $W^o$: ![[Pasted image 20250118193252.png]]
