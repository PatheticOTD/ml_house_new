Энтропия - это среднее значение информации, которую мы получим от выборки:
$$
Entropy = \sum\limits_{i=1}^n{P_i(x)\log{\frac{1}{P_i(x)}}}
 = -\sum\limits{P(x)\log{P(x)}}
 $$                                                (1)

Информацию с одного класса высчитывают по формуле:
$log{(1/P_i(x))}$. Логарифм берется для того, чтобы при высокой вероятности класса, информация от него будыла низкой, и наоборот. А еще логарифм будет давать меншие числа. Далее логарифм умножается на вероятность появления класса и на колличество элементов в выборке:
$(N * P_i(x))*log(\frac{1}{P_i(x)})$ 
N * P_i - дает нам колличество элементов итого класса. Далее, при суммировании всех классов получаем сумму информации. Для получения среднего делим на N, после чего в формуле все N сокращаются и получается формула (1).