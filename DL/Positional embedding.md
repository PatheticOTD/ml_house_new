это отдельные эмбеддинги, которые моделька учит вместе с ембедингами слов.

Для трансформера, авторы статьи предложили кодировать позиции следующим образом: ![[Pasted image 20250120113717.png]]

где t - позиция слова в предложении. 
По итогу p_t будет вот таким: 
![[Pasted image 20250120114101.png]]

Авторы хотят при помощи вектора одной и той же нормы описать позицию токена. 