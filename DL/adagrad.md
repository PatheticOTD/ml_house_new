![[Screenshot from 2024-08-05 17-52-38.png]]

где 
$\eta$- learning rate
$\epsilon$ - стабилизатор, который дает уверенность в том, что мы не разделим на ноль"


Подход адаграда схож с другими методами убывающего изменения learnin rate-а
Главная проблема таких методов состоит в том, что они могут просто не добраться до минимума, если он очень далеко, из-за убывания лернинг рейта. 
Адаград будет менять каждый вес в зависимости от того, насколько этот вес изменялся во время обучения, то есть сильно меняющиеся веса он будет замедлять, а слабо  меняющиеся ускорять:
![[Screenshot from 2024-08-05 18-23-23.png]]

w2 меняется больше чем w1

и всёже, при очень далеком минимуме адаград не может так быстро его достигать, как тот же sgd, ибо шаги обучения адаграда будут неускоряющимися